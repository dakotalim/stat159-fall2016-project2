---
output: pdf_document
---


```{r eval=TRUE, echo=FALSE, message=FALSE}
library(xtable)
library(Matrix)
###############################################
# temporarily load data from REPORT DIRECTORY #
# this is for initially writting the file     #
# change to load from TOP DIRECTORY when done #
###############################################

load(file = "../../data/lm-model.RData")
load(file = "../../data/ridge-models.RData")
load(file = "../../data/lasso-models.RData")
load(file = "../../data/pls-models.RData")
load(file = "../../data/pcr-models.RData")

# read in original data, tests and training sets as well
data = as.matrix(read.csv("../../data/scaled-credit.csv", row.names = 1))
x.train = as.matrix(read.csv("../../data/xtrain.csv", row.names = 1))
y.train = as.matrix(read.csv("../../data/ytrain.csv", row.names = 1))
x.test = as.matrix(read.csv("../../data/xtest.csv", row.names = 1))
y.test = as.matrix(read.csv("../../data/ytest.csv", row.names = 1))
```

Below is a table of the regression coefficients produced by each model on the full data along with a plot of all model coefficients:

```{r xtable, results = "asis", echo = FALSE, eval=TRUE, message=FALSE}
ridgeCoefs = as.matrix(ridgeModel$beta)
lassoCoefs = as.matrix(lassoModel$beta)
plsCoefs = as.matrix(plsModel$coefficients[,,bestPLS])
pcrCoefs = as.matrix(pcrModel$coefficients[,,bestPCR])
lmCoefs = as.matrix(linearModel$coefficients)
coefMatrix = cbind(lmCoefs, ridgeCoefs, lassoCoefs, plsCoefs, pcrCoefs)
colnames(coefMatrix) = c("lm", "ridge", "lasso", "pls", "pcr")
coefTable = xtable(coefMatrix, caption = "Model Coefficients")
print(coefTable, type = "latex", caption.placement = "top", comment = FALSE)


plot(coefMatrix[,1], type = "b", col = "Blue", xaxt = "n", cex = .5, pch = 10, xlab = "", ylab = "Value")
abline(0,0, col = "grey")
points(coefMatrix[,2], type = "b", col = "Dark Red", cex = .5, pch = 10)
points(coefMatrix[,3], type = "b", col = "Dark Green", cex = .5, pch = 10)
points(coefMatrix[,4], type = "b", col = "Pink", cex = .5, pch = 10)
points(coefMatrix[,5], type = "b", col = "Black", cex = .5, pch = 10)
legend("top",legend = c("lm", "ridge", "lasso", "pca", "pls"), 
       fill = c("Blue", "Dark Red", "Dark Green", "Pink", "Black"))
text(seq(1, 11, by=1), par("usr")[3] - 0.2, labels = colnames(data)[1:11], srt = 75, pos = 1, xpd = TRUE, cex = .65)

```

The majority of the models produce similar values for each regressor, with the obvious exception of the ridge and lasso methods. The lasso coefficients for `Education`, `GenderFemale`, `MarriedYes`, `EthnicityAsian`, and `EthnicityCaucasian` are all 0, this is a direct result of the lasso penalty term being the L-1 norm of the Betas scaled by some lambda value. The ridge method appears to be giving much lower estimates of the `Limit` coefficient and higher values of the `Rating` coefficient.

From this we deduce that `Education`, `GenderFemale`, `MarriedYes`, `EthnicityAsian`, and `EthnicityCaucasian` are all *meaningless* in our lasso regression. Reducing the number of regressors in our model allows us to easily interpret the results and increases the overall effectiveness of our model. This is why we consider lasso-regression to be a shrinkage method, since it tends (as the number of regressors increases) to send the coefficients of regressors with little predictive power to 0. In short, the lasso-regression identified the `Education`, `GenderFemale`, `MarriedYes`, `EthnicityAsian`, and `EthnicityCaucasian` as unimportant variables and set their regression coefficients to 0.

The second shrinkage method is ridge-regression. In contrast to the lasso, the ridge pentaly term uses the L-2 norm of the Betas. While this may seem arbitrary, this reduces the probability of unimportant regressor coefficients being 0. Upon further inspection we note that the `Education`, `GenderFemale`, `MarriedYes`, `EthnicityAsian`, and `EthnicityCaucasian` regressors have comparatively low coefficient magnitueds, which further supports the thoery that these variables are less important to our regression. This leads us to believe both the Ridge and Lasso models will yield similar results.

Next we consider the coefficients of the two dimmension reduction methods: Partical Component Regression (PCR) and Partial Least Squares (PLS). Both these methods produce nearly identical coefficeints, which we expect since they both use principle components effectively remove the correlation between any given regressors. Furthermore, PCR and PLS use similar numbers of components (10 and 9 respectively).
