---
output: pdf_document
---

We start our analysis by first conducting basic Exploratory Data Analysis to understand the variables we are working with and then following on with Pre-modeling Data Processing to structure the data in such a way that allows us to fit the multiple models on this set.

For EDA, we consider the summary statistics and use some basic plots to understand the variables. For qualitative variables, we create fequency tables and chart these proportions. Given that we want to understand the relationship between variables, we plot a matrix of correlation for the quantitative variables, ANOVA between Balance and qualitative variables along with conditional boxplots for the same.

For Pre-modelling Data Processing, we follow the two major steps:
1. Coverting Factors into Dummy Variables so that they can be used in the regression.
2. Mean Centering and Standarization of Variables - This is done because the different models can only be compared when their coefficients represent the same scales. Hence, we mean center all the variables to 0 and the adjust the standard deviation to 1. Doing this given us comparable scales. 
Finally, we randomize this scaled data and take out a training set of size 3/4 of the data and the test set which is size 1/4 of the data. 

Moving to the 5 regression models that we use:

* An OLS model on the training data
* A Ridge model, using 10-fold cross validation to select lambda, on the training data
* A Lasso model, using 10-fold cross validation to select lambda, on the training data
* A PCR model, using 10-fold cross validation to select the number of components, on the training data
* A PLSR model, using 10-fold cross validation to select the number of components, on the training data

Each model was built using a scaled and centered version of the raw data. To ensure consistency we divide the data into test/training sets once and use the same sets to train and test each model.

Each model (excluding OLS) required a tuning parameter be definied, lambda values for Ridge and Lasso, and the number of components for PCR and PLS. To decide thse tuning parameters we used cross validation, below are the plots of prediction intervals for the MSE of the Ridge and Lasso methods as a function of different lambda values:

\begin{center} 
\includegraphics[width=0.4\textwidth]{../images/ridgeCV-plot.png} \includegraphics[width=0.4\textwidth]{../images/lassoCV-plot.png}
\end{center}

Next we consider the plots for MSEP as a function of the number of components taken in the PCR and PLS methods:

\begin{center} 
\includegraphics[width=0.4\textwidth]{../images/pcrCV-plot.png} \includegraphics[width=0.4\textwidth]{../images/plsCV-plot.png}
\end{center}

We simply take the minimal value (lambda or #-components depending on the model) of each plot and use that value as the tuning parameter for our full models.

Before we begin analyzing the coefficients from the regression models, we first take a look at the data. 

\begin{center} 
\includegraphics[width=0.30\textwidth]{../images/balance-plots.png} \includegraphics[width=0.30\textwidth]{../images/limit-plots.png}
\includegraphics[width=0.30\textwidth]{../images/cards-plots.png}
\end{center}

\begin{center} 
\includegraphics[width=0.23\textwidth]{../images/rating-plots.png} \includegraphics[width=0.23\textwidth]{../images/age-plots.png}
\includegraphics[width=0.23\textwidth]{../images/education-plots.png}
\includegraphics[width=0.23\textwidth]{../images/income-plots.png}
\end{center}

\begin{center} 
\includegraphics[width=0.23\textwidth]{../images/ethnicity-boxplot.png} \includegraphics[width=0.23\textwidth]{../images/gender-boxplot.png}
\includegraphics[width=0.23\textwidth]{../images/student-boxplot.png}
\includegraphics[width=0.23\textwidth]{../images/married-boxplot.png}
\end{center}

Now to understand the relationship between each variable, we study the correlation matrix.
\begin{center} 
\includegraphics[width=0.5\textwidth]{../images/quant-scatterplot.png} 
\end{center}


